{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40128828",
   "metadata": {},
   "source": [
    "# Implementing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f662d73",
   "metadata": {},
   "source": [
    "he World Health Organization (WHO)’s Global Health Observatory (GHO) data repository tracks life expectancy for countries worldwide by following health status and many other related factors.\n",
    "\n",
    "Although there have been a lot of studies undertaken in the past on factors affecting life expectancy considering demographic variables, income composition, and mortality rates, it was found that the effects of immunization and human development index were not taken into account.\n",
    "\n",
    "This dataset covers a variety of indicators for all countries from 2000 to 2015 including:\n",
    "\n",
    "immunization factors\n",
    "mortality factors\n",
    "economic factors\n",
    "social factors\n",
    "other health-related factors\n",
    "Ideally, this data will eventually inform countries concerning which factors to change in order to improve the life expectancy of their populations. If we can predict life expectancy well given all the factors, this is a good sign that there are some important patterns in the data. Life expectancy is expressed in years, and hence it is a number. This means that in order to build a predictive model one needs to use regression.\n",
    "\n",
    "In this project, you will design, train, and evaluate a neural network model performing the task of regression to predict the life expectancy of countries using this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ab9f7",
   "metadata": {},
   "source": [
    "Load the life_expectancy.csv dataset into a pandas DataFrame by first importing pandas, and then using the pandas.read_csv() function to load the file and assign the resulting DataFrame to a variable called dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3e2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset=pd.read_csv('life_expectancy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545db9cf",
   "metadata": {},
   "source": [
    "Observe the data by printing the first five entries in the DataFrame dataset by using the DataFrame.head() function. Make sure to see what the columns are and what the data types are. Locate the feature we would like to predict: life expectancy. Use DataFrame.describe() to see the summary statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139ce189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country  Year      Status  Adult Mortality  infant deaths  Alcohol  \\\n",
      "0  Afghanistan  2015  Developing            263.0             62     0.01   \n",
      "1  Afghanistan  2014  Developing            271.0             64     0.01   \n",
      "2  Afghanistan  2013  Developing            268.0             66     0.01   \n",
      "3  Afghanistan  2012  Developing            272.0             69     0.01   \n",
      "4  Afghanistan  2011  Developing            275.0             71     0.01   \n",
      "\n",
      "   percentage expenditure  Hepatitis B  Measles    BMI   ...  \\\n",
      "0               71.279624         65.0      1154   19.1  ...   \n",
      "1               73.523582         62.0       492   18.6  ...   \n",
      "2               73.219243         64.0       430   18.1  ...   \n",
      "3               78.184215         67.0      2787   17.6  ...   \n",
      "4                7.097109         68.0      3013   17.2  ...   \n",
      "\n",
      "   Total expenditure  Diphtheria    HIV/AIDS         GDP  Population  \\\n",
      "0               8.16         65.0        0.1  584.259210  33736494.0   \n",
      "1               8.18         62.0        0.1  612.696514    327582.0   \n",
      "2               8.13         64.0        0.1  631.744976  31731688.0   \n",
      "3               8.52         67.0        0.1  669.959000   3696958.0   \n",
      "4               7.87         68.0        0.1   63.537231   2978599.0   \n",
      "\n",
      "    thinness  1-19 years   thinness 5-9 years  \\\n",
      "0                   17.2                 17.3   \n",
      "1                   17.5                 17.5   \n",
      "2                   17.7                 17.7   \n",
      "3                   17.9                 18.0   \n",
      "4                   18.2                 18.2   \n",
      "\n",
      "   Income composition of resources  Schooling  Life expectancy  \n",
      "0                            0.479       10.1             65.0  \n",
      "1                            0.476       10.0             59.9  \n",
      "2                            0.470        9.9             59.9  \n",
      "3                            0.463        9.8             59.5  \n",
      "4                            0.454        9.5             59.2  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "              Year  Adult Mortality  infant deaths      Alcohol  \\\n",
      "count  2938.000000      2938.000000    2938.000000  2938.000000   \n",
      "mean   2007.518720       164.725664      30.303948     4.546875   \n",
      "std       4.613841       124.086215     117.926501     3.921946   \n",
      "min    2000.000000         1.000000       0.000000     0.010000   \n",
      "25%    2004.000000        74.000000       0.000000     1.092500   \n",
      "50%    2008.000000       144.000000       3.000000     3.755000   \n",
      "75%    2012.000000       227.000000      22.000000     7.390000   \n",
      "max    2015.000000       723.000000    1800.000000    17.870000   \n",
      "\n",
      "       percentage expenditure  Hepatitis B       Measles          BMI   \\\n",
      "count             2938.000000  2938.000000    2938.000000  2938.000000   \n",
      "mean               738.251295    83.022124    2419.592240    38.381178   \n",
      "std               1987.914858    22.996984   11467.272489    19.935375   \n",
      "min                  0.000000     1.000000       0.000000     1.000000   \n",
      "25%                  4.685343    82.000000       0.000000    19.400000   \n",
      "50%                 64.912906    92.000000      17.000000    43.500000   \n",
      "75%                441.534144    96.000000     360.250000    56.100000   \n",
      "max              19479.911610    99.000000  212183.000000    87.300000   \n",
      "\n",
      "       under-five deaths         Polio  Total expenditure  Diphtheria   \\\n",
      "count         2938.000000  2938.000000        2938.000000  2938.000000   \n",
      "mean            42.035739    82.617767           5.924098    82.393125   \n",
      "std            160.445548    23.367166           2.400770    23.655562   \n",
      "min              0.000000     3.000000           0.370000     2.000000   \n",
      "25%              0.000000    78.000000           4.370000    78.000000   \n",
      "50%              4.000000    93.000000           5.755000    93.000000   \n",
      "75%             28.000000    97.000000           7.330000    97.000000   \n",
      "max           2500.000000    99.000000          17.600000    99.000000   \n",
      "\n",
      "          HIV/AIDS            GDP    Population   thinness  1-19 years  \\\n",
      "count  2938.000000    2938.000000  2.938000e+03            2938.000000   \n",
      "mean      1.742103    6611.523863  1.023085e+07               4.821886   \n",
      "std       5.077785   13296.603449  5.402242e+07               4.397621   \n",
      "min       0.100000       1.681350  3.400000e+01               0.100000   \n",
      "25%       0.100000     580.486996  4.189172e+05               1.600000   \n",
      "50%       0.100000    1766.947595  1.386542e+06               3.300000   \n",
      "75%       0.800000    4779.405190  4.584371e+06               7.100000   \n",
      "max      50.600000  119172.741800  1.293859e+09              27.700000   \n",
      "\n",
      "        thinness 5-9 years  Income composition of resources    Schooling  \\\n",
      "count          2938.000000                      2938.000000  2938.000000   \n",
      "mean              4.852144                         0.630362    12.009837   \n",
      "std               4.485854                         0.205140     3.265139   \n",
      "min               0.100000                         0.000000     0.000000   \n",
      "25%               1.600000                         0.504250    10.300000   \n",
      "50%               3.300000                         0.677000    12.300000   \n",
      "75%               7.200000                         0.772000    14.100000   \n",
      "max              28.600000                         0.948000    20.700000   \n",
      "\n",
      "       Life expectancy  \n",
      "count      2938.000000  \n",
      "mean         69.234717  \n",
      "std           9.509115  \n",
      "min          36.300000  \n",
      "25%          63.200000  \n",
      "50%          72.100000  \n",
      "75%          75.600000  \n",
      "max          89.000000  \n"
     ]
    }
   ],
   "source": [
    "print(dataset.head())\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54517fe",
   "metadata": {},
   "source": [
    "Drop the Country column from the DataFrame using the DataFrame drop method. Why? To create a predictive model, knowing from which country data comes can be confusing and it is not a column we can generalize over. We want to learn a general pattern for all the countries, and not only those dependent on specific countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62a17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['Country'], axis = 1)\n",
    "\n",
    "labels=dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d12ac",
   "metadata": {},
   "source": [
    "In the next two steps, you will split the data into labels and features. Labels are contained in the “Life expectancy” column. It’s the final column in the DataFrame. Create a new variable called labels. Use iloc indexing to assign the final column of dataset to it.\n",
    "\n",
    "\n",
    "Features span from the first column to the last column (not including it, since it’s a label column in our dataset). Create a new variable called features. Use iloc indexing to assign a subset of columns from first to last (not including the last column) to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa320e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=dataset.iloc[:,0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61cb5a0",
   "metadata": {},
   "source": [
    "When you observed your dataset you probably noticed that some columns are categorical. We learned in this lesson that categorical columns need to be converted into numerical columns using methods such as one-hot-encoding. Use pandas.get_dummies(DataFrame) to apply one-hot-encoding on all the categorical columns. Assign the result of the encoding back to the features variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab2fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a022e",
   "metadata": {},
   "source": [
    "Split your data into training set and test sets using the sklearn.model_selection.train_test_split() function. Assign\n",
    "\n",
    "the training features to a variable called features_train\n",
    "training labels to a variable called labels_train\n",
    "test data to a variable called features_test\n",
    "test labels to a variable called labels_test.\n",
    "You can choose any percentage for test_size and any value for random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a8e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c46d0b",
   "metadata": {},
   "source": [
    "\n",
    "The next step is to standardize/normalize your numerical features. You can pick whichever method you want. In this step, create a sklearn.compose.ColumnTransformer instance called ct to set up the normalization/standardization procedure. When initializing ColumnTransformer make sure to list all of the numerical features you have in your dataset. Or use DataFrame.select_dtypes() to select float64 or int64 feature types automatically.\n",
    "\n",
    "Fit your instance ct of ColumnTransformer to the training data and at the same time transform it by using the ColumnTransformer.fit_transform() method. Assign the result to a variable called features_train_scaled.\n",
    "\n",
    "Transform your test data instance features_test using the trained ColumnTransformer instance ct. Assign the result to a variable called features_test_scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d29d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_features = features.select_dtypes(include=['float64', 'int64'])\n",
    "numerical_columns = numerical_features.columns\n",
    " \n",
    "ct = ColumnTransformer([(\"only numeric\", StandardScaler(), numerical_columns)], remainder='passthrough')\n",
    "\n",
    "features_train_scaled=ct.fit_transform(features_train)\n",
    "features_test_scaled = ct.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987f550",
   "metadata": {},
   "source": [
    "Create an instance of the tensorflow.keras.models.Sequential model called my_model.\n",
    "\n",
    "Create the input layer to the network model using tf.keras.layers.InputLayer with the shape corresponding to the number of features in your dataset. Assign the resulting input layer to a variable called input.\n",
    "\n",
    "Add the input layer from the previous step to the model instance my_model.\n",
    "\n",
    "Add one keras.layers.Dense hidden layer with any number of hidden units you wish. Use the relu activation function.\n",
    "\n",
    "Add a keras.layers.Dense output layer with one neuron since you need a single output for a regression prediction.\n",
    "\n",
    "\n",
    "Print the summary of the model using the Sequential.summary() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e79fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1408      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,473\n",
      "Trainable params: 1,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "my_model = Sequential()\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "input = InputLayer(input_shape = (features.shape[1], ))\n",
    "\n",
    "my_model.add(input)\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "my_model.add(Dense(64, activation = \"relu\"))\n",
    "my_model.add(Dense(1))\n",
    "print(my_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ff07b",
   "metadata": {},
   "source": [
    "Create an instance of the Adam optimizer with the learning rate equal to 0.01.\n",
    "\n",
    "\n",
    "Once your optimizer is initialized, compile the model using the Sequential.compile() method.\n",
    "\n",
    "Assign the following parameters:\n",
    "\n",
    "For loss use the Mean Squared Error (mse)\n",
    "For metrics use the Mean Absolute Error (mae)\n",
    "For opt (the optimizer parameters) use the instance of the optimizer you created in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00da1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "opt = Adam(learning_rate = 0.1)\n",
    "my_model.compile(loss = 'mse', metrics = ['mae'], optimizer = opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b769b",
   "metadata": {},
   "source": [
    "Train your model with the Sequential.fit() method by passing it the following parameters:\n",
    "\n",
    "your preprocessed training data\n",
    "training labels\n",
    "number of epochs equal to 40\n",
    "batch size equal to 1\n",
    "verbose set to 1\n",
    "\n",
    "Using the Sequential.evaluate() method, evaluate your trained model on the preprocessed test data set, and with the test labels. Set verbose to 0. Store the result of the evaluation in two variables: res_mse and res_mae.\n",
    "\n",
    "Print your final loss (RMSE) and final metric (MAE) to check the predictive performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abb871a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2350/2350 [==============================] - 5s 2ms/step - loss: 433.7993 - mae: 11.9613\n",
      "Epoch 2/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 740.6522 - mae: 12.1074\n",
      "Epoch 3/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 76.0890 - mae: 5.1209\n",
      "Epoch 4/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 29.3629 - mae: 4.0385\n",
      "Epoch 5/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 141.4147 - mae: 4.7105\n",
      "Epoch 6/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 58.4191 - mae: 4.1903\n",
      "Epoch 7/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 23.4805 - mae: 3.6528\n",
      "Epoch 8/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 23.4287 - mae: 3.6651\n",
      "Epoch 9/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 22.8816 - mae: 3.6766\n",
      "Epoch 10/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 25.9256 - mae: 3.8326\n",
      "Epoch 11/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 22.0055 - mae: 3.6273\n",
      "Epoch 12/40\n",
      "2350/2350 [==============================] - 4s 1ms/step - loss: 22.1375 - mae: 3.6247\n",
      "Epoch 13/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 20.9168 - mae: 3.5226\n",
      "Epoch 14/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 23.2226 - mae: 3.7130\n",
      "Epoch 15/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 23.5534 - mae: 3.7293\n",
      "Epoch 16/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 21.9098 - mae: 3.6162\n",
      "Epoch 17/40\n",
      "2350/2350 [==============================] - 4s 1ms/step - loss: 22.3494 - mae: 3.6542\n",
      "Epoch 18/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 21.9174 - mae: 3.6546\n",
      "Epoch 19/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 25.2380 - mae: 3.8067\n",
      "Epoch 20/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.7669 - mae: 3.7101\n",
      "Epoch 21/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 23.0602 - mae: 3.7053\n",
      "Epoch 22/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.0126 - mae: 3.6041\n",
      "Epoch 23/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.5778 - mae: 3.7098\n",
      "Epoch 24/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.5261 - mae: 3.6428: 0s - loss: 22.6156\n",
      "Epoch 25/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.2829 - mae: 3.6371\n",
      "Epoch 26/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 23.3086 - mae: 3.7375\n",
      "Epoch 27/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.5510 - mae: 3.6607\n",
      "Epoch 28/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.4135 - mae: 3.7113\n",
      "Epoch 29/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 22.6291 - mae: 3.6287\n",
      "Epoch 30/40\n",
      "2350/2350 [==============================] - 4s 1ms/step - loss: 22.0884 - mae: 3.6369\n",
      "Epoch 31/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 23.6800 - mae: 3.7239: 0s - loss: 23.\n",
      "Epoch 32/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 22.7697 - mae: 3.7272\n",
      "Epoch 33/40\n",
      "2350/2350 [==============================] - 4s 1ms/step - loss: 21.9239 - mae: 3.6189\n",
      "Epoch 34/40\n",
      "2350/2350 [==============================] - 4s 1ms/step - loss: 24.2727 - mae: 3.7379\n",
      "Epoch 35/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.2380 - mae: 3.6365\n",
      "Epoch 36/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.1953 - mae: 3.6143\n",
      "Epoch 37/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.2153 - mae: 3.6674\n",
      "Epoch 38/40\n",
      "2350/2350 [==============================] - 3s 1ms/step - loss: 21.4238 - mae: 3.5812\n",
      "Epoch 39/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 23.1883 - mae: 3.6986\n",
      "Epoch 40/40\n",
      "2350/2350 [==============================] - 4s 2ms/step - loss: 22.1234 - mae: 3.5945\n",
      "22.832307815551758 3.6087281703948975\n"
     ]
    }
   ],
   "source": [
    "my_model.fit(features_train_scaled, labels_train, epochs = 40, batch_size = 1, verbose = 1)\n",
    "\n",
    "res_mse , res_mae = my_model.evaluate(features_test_scaled, labels_test, verbose = 0)\n",
    "\n",
    "print(res_mse, res_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8955972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfae4540",
   "metadata": {},
   "source": [
    "# Deep Learning Regression with Admissions Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c1aa0",
   "metadata": {},
   "source": [
    "For this project, you will create a deep learning regression model that predicts the likelihood that a student applying to graduate school will be accepted based on various application factors (such as test scores).\n",
    "\n",
    "By analyzing the parameters in this graduate admissions dataset, you will use TensorFlow with Keras to create a regression model that can evaluate the chances of an applicant being admitted. You hope this will give you further insight into the graduate admissions world and improve your test prep strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bec27a",
   "metadata": {},
   "source": [
    "If you take a look at admissions_data.csv, you’ll see parameters that admissions officers commonly use to evaluate university applicants. This data is from Kaggle and provides information about 500 applications for various universities and what their chance of admittance is.\n",
    "\n",
    "This is a regression problem because the probability of being admitted is a continuous label between 0 and 1.\n",
    "Load the csv file into a DataFrame and investigate the rows and columns to get familiarity with the dataset.\n",
    "To get more information about each parameter in admissions_data.csv click the hint below.\n",
    "\n",
    "Split it up the data into feature parameters and the labels.\n",
    "You are creating a model that predicts an applicant’s likelihood of being admitted to a master’s program, so take some time to look at the features of your model and which column you are trying to predict. Also consider if there are any dataset features that should not be included as a predictor.\n",
    "Make sure all of your variables are numerical.\n",
    "If there are any categorical variables, be sure to map them to numerical values, using techniques such as one-hot-encoding, so they can be used in a regression analysis.\n",
    "\n",
    "\n",
    "Since you are creating a learning model, you must have a training set and a test set. Remember that this allows you to measure the effectiveness of your model.\n",
    "\n",
    "You have created two DataFrames: one for features DataFrame and one for labels. Now, you must split each of these into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d2173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "                   Serial No.  GRE Score  TOEFL Score  University Rating  \\\n",
      "Serial No.           1.000000  -0.103839    -0.141696          -0.067641   \n",
      "GRE Score           -0.103839   1.000000     0.827200           0.635376   \n",
      "TOEFL Score         -0.141696   0.827200     1.000000           0.649799   \n",
      "University Rating   -0.067641   0.635376     0.649799           1.000000   \n",
      "SOP                 -0.137352   0.613498     0.644410           0.728024   \n",
      "LOR                 -0.003694   0.524679     0.541563           0.608651   \n",
      "CGPA                -0.074289   0.825878     0.810574           0.705254   \n",
      "Research            -0.005332   0.563398     0.467012           0.427047   \n",
      "Chance of Admit      0.008505   0.810351     0.792228           0.690132   \n",
      "\n",
      "                        SOP      LOR       CGPA  Research  Chance of Admit   \n",
      "Serial No.        -0.137352 -0.003694 -0.074289 -0.005332          0.008505  \n",
      "GRE Score          0.613498  0.524679  0.825878  0.563398          0.810351  \n",
      "TOEFL Score        0.644410  0.541563  0.810574  0.467012          0.792228  \n",
      "University Rating  0.728024  0.608651  0.705254  0.427047          0.690132  \n",
      "SOP                1.000000  0.663707  0.712154  0.408116          0.684137  \n",
      "LOR                0.663707  1.000000  0.637469  0.372526          0.645365  \n",
      "CGPA               0.712154  0.637469  1.000000  0.501311          0.882413  \n",
      "Research           0.408116  0.372526  0.501311  1.000000          0.545871  \n",
      "Chance of Admit    0.684137  0.645365  0.882413  0.545871          1.000000  \n",
      "Index(['Serial No.', 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n",
      "       'LOR ', 'CGPA', 'Research'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow\timport keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df=pd.read_csv('admissions_data.csv')\n",
    "# print(df.head())\n",
    "# print(df.describe())\n",
    "print(df.columns[df.isnull().any()])\n",
    "\n",
    "print(df.corr())\n",
    "\n",
    "#splitting into features and labels\n",
    "labels=df.iloc[:,-1]\n",
    "# print(labels.unique())\n",
    "features=df.iloc[:,0:-1]\n",
    "print(features.columns)\n",
    "\n",
    "#split features into training set and test set\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bb7f0",
   "metadata": {},
   "source": [
    "\n",
    "If you look through the admissions_data.csv, you may notice that there are many different scales being used. For example, the GRE Score is out of 340 while the University Rating is out of 5. Can you imagine why this might be a problem when using a regression learning model?\n",
    "\n",
    "You should either scale or normalize your data so that all columns/features have equal weight in the learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc6518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features\n",
    "scaler=StandardScaler()\n",
    "features_train_scaled=scaler.fit_transform(features_train)\n",
    "features_test_scaled=scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf988d2",
   "metadata": {},
   "source": [
    "Create a neural network model to perform a regression analysis on the admission data.\n",
    "\n",
    "When designing your own neural network model, consider the following:\n",
    "\n",
    "The shape of your input\n",
    "Adding hidden layers as well as how many neurons they have\n",
    "Including activation functions\n",
    "The type of loss function and metrics you use\n",
    "The type of gradient descent optimizer you use\n",
    "Your learning rate\n",
    "\n",
    "\n",
    "\n",
    "It’s time to test out the model you created!\n",
    "Fit your model with your training set and test it out with your test set.\n",
    "It’s okay if it is not that accurate right now. You can play around with your model and tweak it to increase its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abbd7fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 641\n",
      "Trainable params: 641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/40\n",
      "320/320 [==============================] - 2s 3ms/step - loss: 2.8231 - mae: 0.7426 - val_loss: 0.0292 - val_mae: 0.1074\n",
      "Epoch 2/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0115 - mae: 0.0806 - val_loss: 0.0130 - val_mae: 0.0851\n",
      "Epoch 3/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0082 - mae: 0.0698 - val_loss: 0.0110 - val_mae: 0.0775\n",
      "Epoch 4/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0072 - mae: 0.0657 - val_loss: 0.0195 - val_mae: 0.0999\n",
      "Epoch 5/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0122 - mae: 0.0862 - val_loss: 0.0131 - val_mae: 0.0890\n",
      "Epoch 6/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0093 - mae: 0.0709 - val_loss: 0.0095 - val_mae: 0.0634\n",
      "Epoch 7/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0069 - mae: 0.0606 - val_loss: 0.0104 - val_mae: 0.0770\n",
      "Epoch 8/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0092 - mae: 0.0757 - val_loss: 0.0179 - val_mae: 0.1116\n",
      "Epoch 9/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0146 - mae: 0.0957 - val_loss: 0.0129 - val_mae: 0.0882\n",
      "Epoch 10/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0088 - mae: 0.0757 - val_loss: 0.0103 - val_mae: 0.0772\n",
      "Epoch 11/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0126 - mae: 0.0884 - val_loss: 0.0280 - val_mae: 0.1454\n",
      "Epoch 12/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 10.2097 - mae: 1.2552 - val_loss: 0.0312 - val_mae: 0.1242\n",
      "Epoch 13/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.3103 - mae: 0.1961 - val_loss: 0.0247 - val_mae: 0.1075\n",
      "Epoch 14/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0193 - mae: 0.1107 - val_loss: 0.0239 - val_mae: 0.1062\n",
      "Epoch 15/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0182 - mae: 0.1089 - val_loss: 0.0322 - val_mae: 0.1299\n",
      "Epoch 16/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0229 - mae: 0.1177 - val_loss: 0.0302 - val_mae: 0.1299\n",
      "Epoch 17/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0199 - mae: 0.1139 - val_loss: 0.0268 - val_mae: 0.1181\n",
      "Epoch 18/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0203 - mae: 0.1141 - val_loss: 0.0261 - val_mae: 0.1122\n",
      "Epoch 19/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0213 - mae: 0.1176 - val_loss: 0.0242 - val_mae: 0.1110\n",
      "Epoch 20/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0227 - mae: 0.1199 - val_loss: 0.0312 - val_mae: 0.1194\n",
      "Epoch 21/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0226 - mae: 0.1222 - val_loss: 0.0263 - val_mae: 0.1186\n",
      "Epoch 22/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0229 - mae: 0.1223 - val_loss: 0.0246 - val_mae: 0.1137\n",
      "Epoch 23/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0250 - mae: 0.1281 - val_loss: 0.0254 - val_mae: 0.1162\n",
      "Epoch 24/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0240 - mae: 0.1251 - val_loss: 0.0241 - val_mae: 0.1111\n",
      "Epoch 25/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0238 - mae: 0.1251 - val_loss: 0.0262 - val_mae: 0.1166\n",
      "Epoch 26/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.1159 - mae: 0.1807 - val_loss: 0.0255 - val_mae: 0.1178\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0285 - mae: 0.1359 - val_loss: 0.0241 - val_mae: 0.1113\n",
      "Epoch 2/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0258 - mae: 0.1294 - val_loss: 0.0241 - val_mae: 0.1117\n",
      "Epoch 3/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0247 - mae: 0.1279 - val_loss: 0.0307 - val_mae: 0.1339\n",
      "Epoch 4/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0281 - mae: 0.1382 - val_loss: 0.0235 - val_mae: 0.1104\n",
      "Epoch 5/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0266 - mae: 0.1313 - val_loss: 0.0411 - val_mae: 0.1665\n",
      "Epoch 6/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0267 - mae: 0.1328 - val_loss: 0.0363 - val_mae: 0.1519\n",
      "Epoch 7/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0732 - mae: 0.1618 - val_loss: 0.0368 - val_mae: 0.1474\n",
      "Epoch 8/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0265 - mae: 0.1315 - val_loss: 0.0249 - val_mae: 0.1152\n",
      "Epoch 9/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0274 - mae: 0.1346 - val_loss: 0.0246 - val_mae: 0.1130\n",
      "Epoch 10/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0272 - mae: 0.1335 - val_loss: 0.0236 - val_mae: 0.1106\n",
      "Epoch 11/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0263 - mae: 0.1310 - val_loss: 0.0273 - val_mae: 0.1213\n",
      "Epoch 12/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0258 - mae: 0.1294 - val_loss: 0.0242 - val_mae: 0.1119\n",
      "Epoch 13/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0269 - mae: 0.1340 - val_loss: 0.0290 - val_mae: 0.1278\n",
      "Epoch 14/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0258 - mae: 0.1282 - val_loss: 0.0315 - val_mae: 0.1364\n",
      "Epoch 15/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0265 - mae: 0.1325 - val_loss: 0.0428 - val_mae: 0.1710\n",
      "Epoch 16/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0300 - mae: 0.1361 - val_loss: 0.0298 - val_mae: 0.1290\n",
      "Epoch 17/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0290 - mae: 0.1382 - val_loss: 0.0289 - val_mae: 0.1274\n",
      "Epoch 18/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0278 - mae: 0.1362 - val_loss: 0.0457 - val_mae: 0.1787\n",
      "Epoch 19/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0279 - mae: 0.1352 - val_loss: 0.0537 - val_mae: 0.1972\n",
      "Epoch 20/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0277 - mae: 0.1338 - val_loss: 0.0240 - val_mae: 0.1115\n",
      "Epoch 21/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0310 - mae: 0.1440 - val_loss: 0.0262 - val_mae: 0.1174\n",
      "Epoch 22/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0272 - mae: 0.1354 - val_loss: 0.0264 - val_mae: 0.1199\n",
      "Epoch 23/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0261 - mae: 0.1335 - val_loss: 0.0235 - val_mae: 0.1104\n",
      "Epoch 24/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0261 - mae: 0.1312 - val_loss: 0.0237 - val_mae: 0.1108\n",
      "Epoch 25/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0288 - mae: 0.1399 - val_loss: 0.0243 - val_mae: 0.1119\n",
      "Epoch 26/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0294 - mae: 0.1375 - val_loss: 0.0235 - val_mae: 0.1109\n",
      "Epoch 27/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0280 - mae: 0.1360 - val_loss: 0.0235 - val_mae: 0.1109\n",
      "Epoch 28/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0273 - mae: 0.1349 - val_loss: 0.0238 - val_mae: 0.1109\n",
      "Epoch 29/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0275 - mae: 0.1340 - val_loss: 0.0281 - val_mae: 0.1248\n",
      "Epoch 30/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0243 - mae: 0.1271 - val_loss: 0.0249 - val_mae: 0.1154\n",
      "Epoch 31/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0240 - mae: 0.1273 - val_loss: 0.0469 - val_mae: 0.1815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0262 - mae: 0.1302 - val_loss: 0.0473 - val_mae: 0.1827\n",
      "Epoch 33/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0268 - mae: 0.1320 - val_loss: 0.0234 - val_mae: 0.1104\n",
      "Epoch 34/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0283 - mae: 0.1373 - val_loss: 0.0251 - val_mae: 0.1141\n",
      "Epoch 35/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0275 - mae: 0.1335 - val_loss: 0.0654 - val_mae: 0.2218\n",
      "Epoch 36/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0282 - mae: 0.1337 - val_loss: 0.0234 - val_mae: 0.1104\n",
      "Epoch 37/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0295 - mae: 0.1392 - val_loss: 0.0247 - val_mae: 0.1130\n",
      "Epoch 38/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0281 - mae: 0.1366 - val_loss: 0.0234 - val_mae: 0.1105\n",
      "Epoch 39/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0286 - mae: 0.1382 - val_loss: 0.0289 - val_mae: 0.1265\n",
      "Epoch 40/40\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.0267 - mae: 0.1322 - val_loss: 0.0284 - val_mae: 0.1255\n",
      "0.07705709338188171 0.14807042479515076\n"
     ]
    }
   ],
   "source": [
    "# neural network model for regression analysis\n",
    "my_model = Sequential()\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "input = InputLayer(input_shape = (features.shape[1], ))\n",
    "\n",
    "my_model.add(input)\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "my_model.add(Dense(64, activation = \"relu\"))\n",
    "my_model.add(Dense(1))\n",
    "print(my_model.summary())\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "opt = Adam(learning_rate = 0.1)\n",
    "my_model.compile(loss = 'mse', metrics = ['mae'], optimizer = opt)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 20)\n",
    "my_model.fit(features_train_scaled, labels_train, epochs = 40, batch_size = 1, verbose = 1,validation_split = 0.2, callbacks = [es])\n",
    "\n",
    "history=my_model.fit(features_train_scaled, labels_train, epochs = 40, batch_size = 1, verbose = 1,validation_split = 0.2, callbacks = [es])\n",
    "\n",
    "res_mse , res_mae = my_model.evaluate(features_test_scaled, labels_test, verbose = 0)\n",
    "\n",
    "print(res_mse, res_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381b08a",
   "metadata": {},
   "source": [
    "Using the Matplotlib Library , see if you can plot the model loss per epoch as well as the mean-average error per epoch for both training and validation data. This will give you an insight into how the model performs better over time and can also help you figure out better ways to tune your hyperparameters.\n",
    "\n",
    "Because of the way Matplotlib plots are displayed in the learning environment, please use fig.savefig('static/images/my_plots.png') at the end of your graphing code to render the plot in the browser. If you wish to display multiple plots, you can use .subplot() or .add_subplot() methods in the Matplotlib library to depict multiple plots in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabbfcf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AMALIT~1\\AppData\\Local\\Temp/ipykernel_5284/54571365.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot loss and val_loss over each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0max2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fig' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot loss and val_loss over each epoch\n",
    "ax2 = fig.add_subplot(2, 1, 2)\n",
    "ax2.plot(history.history['loss'])\n",
    "ax2.plot(history.history['val_loss'])\n",
    "ax2.set_title('model loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend(['train', 'validation'], loc='upper left')\n",
    " \n",
    "# used to keep plots from overlapping each other  \n",
    "fig.tight_layout()\n",
    "fig.savefig('static/images/my_plots.png')\n",
    "#implement R-sqaured score calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7caf449",
   "metadata": {},
   "source": [
    "Let’s say you wanted to evaluate how strongly the features in admissions.csv predict an applicant’s admission into a graduate program. We can use something called an R-squared value. It is also known as the coefficient of determination; feel free to explore more about it here.\n",
    "\n",
    "Basically, we can use this calculation to see how well the features in our regression model make predictions. An R-squared value near close to 1 suggests a well-fit regression model, while a value closer to 0 suggests that the regression model does not fit the data well.\n",
    "\n",
    "See if you can apply this to your model after it has been evaluated using a .predict() method on your features_test_set and the r2_score() function on your labels_test_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27047acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.990320745589194\n"
     ]
    }
   ],
   "source": [
    "predicted_values = my_model.predict(features_test_scaled)\n",
    "\n",
    "print(r2_score(labels_test, predicted_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbeeec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

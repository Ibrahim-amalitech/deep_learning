{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c9a3f94",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1716a4",
   "metadata": {},
   "source": [
    "In this project, you will use a dataset from Kaggle to predict the survival of patients with heart failure from serum creatinine and ejection fraction, and other factors such as age, anemia, diabetes, and so on.\n",
    "Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Heart failure is a common event caused by CVDs, and this dataset contains 12 features that can be used to predict mortality by heart failure.\n",
    "Most cardiovascular diseases can be prevented by addressing behavioral risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity, and harmful alcohol use using population-wide strategies.\n",
    "People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidemia, or already established disease) need early detection and management wherein a machine learning model can be of great help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bbe17",
   "metadata": {},
   "source": [
    "Using pandas.read_csv(), load the data from heart_failure.csv to a pandas DataFrame object. Assign the resulting DataFrame to a variable called data.\n",
    "\n",
    "Use the DataFrame.info() method to print all the columns and their types of the DataFrame instance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9685f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of      Unnamed: 0   age anaemia  creatinine_phosphokinase diabetes  \\\n",
      "0             0  75.0      no                       582       no   \n",
      "1             1  55.0      no                      7861       no   \n",
      "2             2  65.0      no                       146       no   \n",
      "3             3  50.0     yes                       111       no   \n",
      "4             4  65.0     yes                       160      yes   \n",
      "..          ...   ...     ...                       ...      ...   \n",
      "294         294  62.0      no                        61      yes   \n",
      "295         295  55.0      no                      1820       no   \n",
      "296         296  45.0      no                      2060      yes   \n",
      "297         297  45.0      no                      2413       no   \n",
      "298         298  50.0      no                       196       no   \n",
      "\n",
      "     ejection_fraction high_blood_pressure  platelets  serum_creatinine  \\\n",
      "0                   20                 yes  265000.00               1.9   \n",
      "1                   38                  no  263358.03               1.1   \n",
      "2                   20                  no  162000.00               1.3   \n",
      "3                   20                  no  210000.00               1.9   \n",
      "4                   20                  no  327000.00               2.7   \n",
      "..                 ...                 ...        ...               ...   \n",
      "294                 38                 yes  155000.00               1.1   \n",
      "295                 38                  no  270000.00               1.2   \n",
      "296                 60                  no  742000.00               0.8   \n",
      "297                 38                  no  140000.00               1.4   \n",
      "298                 45                  no  395000.00               1.6   \n",
      "\n",
      "     serum_sodium  sex smoking  time  DEATH_EVENT death_event  \n",
      "0             130  yes      no     4            1         yes  \n",
      "1             136  yes      no     6            1         yes  \n",
      "2             129  yes     yes     7            1         yes  \n",
      "3             137  yes      no     7            1         yes  \n",
      "4             116   no      no     8            1         yes  \n",
      "..            ...  ...     ...   ...          ...         ...  \n",
      "294           143  yes     yes   270            0          no  \n",
      "295           139   no      no   271            0          no  \n",
      "296           138   no      no   278            0          no  \n",
      "297           140  yes     yes   280            0          no  \n",
      "298           136  yes     yes   285            0          no  \n",
      "\n",
      "[299 rows x 15 columns]>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('heart_failure.csv')\n",
    "\n",
    "print(data.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc6a8c",
   "metadata": {},
   "source": [
    "Print the distribution of the death_event column in the data DataFrame class using collections.Counter. This is the column you will need to predict.\n",
    "\n",
    "Extract the label column death_event from the data DataFrame and assign the result to a variable called y.\n",
    "\n",
    "Extract the features columns ['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time'] from the DataFrame instance data and assign the result to a variable called x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580999bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'no': 203, 'yes': 96})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(data['death_event']))\n",
    "\n",
    "y=data['death_event']\n",
    "\n",
    "x=data[['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e234ea",
   "metadata": {},
   "source": [
    "Use the pandas.get_dummies() function to convert the categorical features in the DataFrame instance x to one-hot encoding vectors and assign the result back to variable x.\n",
    "\n",
    "Use the sklearn.model_selection.train_test_split() method to split the data into training features, test features, training labels, and test labels, respectively. To the test_size parameter assign the percentage of data you wish to put in the test data, and use any value for the random_state parameter. Store the results of the function to X_train, X_test, Y_train, Y_test variables, making sure you use this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84cac5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = pd.get_dummies(x)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070e617",
   "metadata": {},
   "source": [
    "Initialize a ColumnTransformer object by using StandardScaler to scale the numeric features in the dataset: ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']. Assign the resulting object to a variable called ct.\n",
    "\n",
    "\n",
    "Use the ColumnTransformer.fit_transform() function to train the scaler instance ct on the training data X_train and assign the result back to X_train.\n",
    "\n",
    "\n",
    "Use the ColumnTransformer.transform() to scale the test data instance X_test using the trained scaler ct, and assign the result back to X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee076032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"numeric\", StandardScaler(),['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time'])])\n",
    "\n",
    "X_train = ct.fit_transform(X_train)\n",
    "\n",
    "X_test = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece48448",
   "metadata": {},
   "source": [
    "Initialize an instance of LabelEncoder and assign it to a variable called le.\n",
    "\n",
    "\n",
    "Using the LabelEncoder.fit_transform() function, fit the encoder instance le to the training labels Y_train, while at the same time converting the training labels according to the trained encoder.\n",
    "\n",
    "Using the LabelEncoder.transform() function, encode the test labels Y_test using the trained encoder le.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24a173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Y_train  = le.fit_transform(Y_train.astype(str))\n",
    "Y_test  = le.transform(Y_test.astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c340f3",
   "metadata": {},
   "source": [
    "Using the tensorflow.keras.utils.to_categorical() function, transform the encoded training labels Y_train into a binary vector and assign the result back to Y_train.\n",
    "\n",
    "Using the tensorflow.keras.utils.to_categorical() function, transform the encoded test labels Y_test into a binary vector and assign the result back to Y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2cd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd5444",
   "metadata": {},
   "source": [
    "Initialize a tensorflow.keras.models.Sequential model instance called model.\n",
    "\n",
    "Create an input layer instance of tensorflow.keras.layers.InputLayer and add it to the model instance model using the Model.add() function.\n",
    "\n",
    "Create a hidden layer instance of tensorflow.keras.layers.Dense with relu activation function and 12 hidden neurons, and add it to the model instance model.\n",
    "\n",
    "Create an output layer instance of tensorflow.keras.layers.Dense with a softmax activation function (because of classification) with the number of neurons corresponding to the number of classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6465dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(InputLayer(input_shape=(X_train.shape[1],)))\n",
    "\n",
    "model.add(Dense(12, activation='relu'))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01031206",
   "metadata": {},
   "source": [
    "Using the Model.compile() function, compile the model instance model using the categorical_crossentropy loss, adam optimizer and accuracy as metrics.\n",
    "\n",
    "Using the Model.fit() function, fit the model instance model to the training data X_train and training labels Y_train. Set the number of epochs to 100 and the batch size parameter to 16.\n",
    "\n",
    "Using the Model.evaluate() function, evaluate the trained model instance model on the test data X_test and test labels Y_test. Assign the result to a variable called loss (representing the final loss value) and a variable called acc (representing the accuracy metrics), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c842c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 2ms/step - loss: 0.6097 - accuracy: 0.6699\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.7033\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5619 - accuracy: 0.7464\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.7656\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5237 - accuracy: 0.7847\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.8086\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.8038\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7943\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7943\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7990\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 1.00 - 0s 2ms/step - loss: 0.4531 - accuracy: 0.7990\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.7943\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.7990\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8038\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.8038\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4159 - accuracy: 0.8134\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8086\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8134\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8134\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8230\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8230\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8230\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.8230\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8278\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3760 - accuracy: 0.8325\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8278\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3705 - accuracy: 0.8278\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8278\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8278\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3641 - accuracy: 0.8278\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8373\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8373\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8373\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8373\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.8373\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3543 - accuracy: 0.8421\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3530 - accuracy: 0.8421\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8421\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3510 - accuracy: 0.8421\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3502 - accuracy: 0.8469\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8469\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3484 - accuracy: 0.8469\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3478 - accuracy: 0.8469\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8469\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8469\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8469\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8469\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8469\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8469\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8517\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8421\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3424 - accuracy: 0.8469\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8469\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8517\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.8517\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8517\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8517\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8517\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8565\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8565\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8565\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8565\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8565\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8565\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8565\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8565\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8565\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8565\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8517\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8517\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8517\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8517\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.8517\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8517\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8517\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8517\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8517\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8517\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8469\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8421\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8421\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8421\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3311 - accuracy: 0.8517\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8517\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8517\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8517\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.8517\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3294 - accuracy: 0.8517\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8517\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8565\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3280 - accuracy: 0.8565\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8565\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3275 - accuracy: 0.8565\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 0.8565\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8565\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8565\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3265 - accuracy: 0.8517\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3262 - accuracy: 0.8565\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.8565\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8565\n",
      "Loss 0.414075642824173 Accuracy: 0.8222222328186035\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train , Y_train, epochs = 100, batch_size = 16, verbose=1)\n",
    "\n",
    "loss, acc = model.evaluate(X_test , Y_test, verbose=0)\n",
    "print(\"Loss\", loss, \"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81234f6",
   "metadata": {},
   "source": [
    "Use the Model.predict() to get the predictions for the test data X_test with the trained model instance model. Assign the result to a variable called y_estimate.\n",
    "\n",
    "Use the numpy.argmax() method to select the indices of the true classes for each label encoding in y_estimate. Assign the result to a variable called y_estimate.\n",
    "\n",
    "Use the numpy.argmax() method to select the indices of the true classes for each label encoding in Y_test. Assign the result to a variable called y_true.\n",
    "\n",
    "Print additional metrics, such as F1-score, using the sklearn.metrics.classification_report() function by providing it with y_true and y_estimate vectors as input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e0ba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        72\n",
      "           1       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           1.00        90\n",
      "   macro avg       1.00      1.00      1.00        90\n",
      "weighted avg       1.00      1.00      1.00        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_estimate = model.predict(X_test, verbose=0)\n",
    "y_estimate = np.argmax(y_estimate, axis = 1)\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_estimate, y_estimate))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044698de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
